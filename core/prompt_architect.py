# -*- coding: utf-8 -*-
"""
=============================================================================
æ¨¡çµ„åç¨± (Module Name): core/prompt_architect.py
åŠŸèƒ½èªªæ˜ (Description): AI æ¶æ§‹å¸«æ¨¡çµ„ (Architect Mode)ï¼Œè² è²¬åˆ†ææ•™ç§‘æ›¸ä¾‹é¡Œèˆ‡æŠ€èƒ½éœ€æ±‚ï¼Œè¨­è¨ˆä¸¦ç”Ÿæˆçµ¦ Coder AI ä½¿ç”¨çš„è©³ç´° Python å¯¦ä½œè¦æ ¼æ›¸ (Spec)ã€‚
åŸ·è¡Œèªæ³• (Usage): ç”±ç³»çµ±èª¿ç”¨
ç‰ˆæœ¬è³‡è¨Š (Version): V9.3 (Elite Hardening + Timestamp Fix)
æ›´æ–°æ—¥æœŸ (Date): 2026-01-13
ç¶­è­·åœ˜éšŠ (Maintainer): Math AI Project Team
=============================================================================
"""
# ==============================================================================

import json, re, ast
from datetime import datetime # [ä¿®æ­£] å¿…é ˆå°å…¥ datetime
from models import db, SkillInfo, TextbookExample, SkillGenCodePrompt
from core.ai_wrapper import get_ai_client
from config import Config

def generate_v9_spec(skill_id, model_tag='cloud_pro', prompt_strategy='standard', architect_model='human'):
    print(f"--- [Architect v9.3] Analyzing {skill_id} for '{model_tag}' (Elite Mode) ---")
    skill = SkillInfo.query.filter_by(skill_id=skill_id).first()
    if not skill: return {'success': False, 'message': 'Skill not found'}

    # 1. æŠ“å–å…¨é‡ä¾‹é¡Œ
    all_examples = TextbookExample.query.filter_by(skill_id=skill_id).order_by(TextbookExample.id).all()
    if not all_examples: return {'success': False, 'message': 'No examples found'}
    selected_examples = all_examples[:12]
    rag_text = "".join([f"Example {i+1}:\nQ: {getattr(ex, 'problem_text', 'N/A')}\nA: {getattr(ex, 'correct_answer', 'N/A')}\n\n" for i, ex in enumerate(selected_examples)])

    # 2. å®šç¾©åˆ†ç´šç­–ç•¥
    if model_tag == 'edge_7b':
        tier_scope = "Consolidate all examples into ONE single, highly representative function. Keep logic flat and simple."
    elif model_tag == 'local_14b':
        tier_scope = "Consolidate examples into MAX 3 distinct problem types (e.g., Calculation, Concept, Application)."
    else: # cloud_pro
        tier_scope = "Create a rich variety of problem types covering all nuances of the examples."

    # 3. ç³»çµ±æŒ‡ä»¤ (V9.6 çµ‚æ¥µè‡ªå‹•åŒ–ç‰ˆ)
    system_instruction = """ã€ä»»å‹™ã€‘ï¼šæ“”ä»» K12 æ•¸å­¸ AI é¦–å¸­ç³»çµ±æ¶æ§‹å¸« (V9.6 çµ‚æ¥µè‡ªå‹•åŒ–ç‰ˆ)

ä½ å¿…é ˆç”¢å‡ºç¬¦åˆä»¥ä¸‹è¦ç¯„çš„ Coder Specï¼Œç¢ºä¿ç”¢å‡ºçš„ Python ç¨‹å¼ç¢¼èƒ½è‡ªå‹•åŸ·è¡Œä¸”æ’ç‰ˆå®Œç¾ï¼š

1. ç¨‹å¼çµæ§‹ (Structure Hardening)
- [é ‚å±¤å‡½å¼]ï¼šåš´ç¦ä½¿ç”¨ class å°è£ã€‚å¿…é ˆç›´æ¥å®šç¾© generate(level=1) èˆ‡ check(user_answer, correct_answer) æ–¼æ¨¡çµ„æœ€å¤–å±¤ã€‚
- [è‡ªå‹•é‡è¼‰]ï¼šç¢ºä¿ä»£ç¢¼ä¸ä¾è³´å…¨åŸŸç‹€æ…‹ï¼Œä»¥ä¾¿ç³»çµ±åŸ·è¡Œ importlib.reloadã€‚

2. é¡Œå‹å¤šæ¨£æ€§ (Problem Variety)
- [éš¨æ©Ÿåˆ†æµ]ï¼šgenerate() å…§éƒ¨å¿…é ˆä½¿ç”¨ random.choice æˆ– if/elif é‚è¼¯ï¼Œæ ¹æ“šè©²æŠ€èƒ½çš„æ•™ç§‘æ›¸ä¾‹é¡Œï¼Œå¯¦ä½œè‡³å°‘ 3 ç¨®ä¸åŒçš„é¡Œå‹è®Šé«”ã€‚
- [ç¯„ä¾‹]ï¼šé¡Œå‹æ‡‰åŒ…å«ã€Œç›´æ¥è¨ˆç®—ã€ã€ã€Œé€†å‘æ±‚è§£ï¼ˆå·²çŸ¥è·é›¢æ±‚åº§æ¨™ï¼‰ã€ã€ã€Œæƒ…å¢ƒæ‡‰ç”¨ï¼ˆå¦‚ç§»å‹•é»ï¼‰ã€ã€‚

3. æ’ç‰ˆèˆ‡ LaTeX å®‰å…¨ (Layout Guardrails)
- [ç¦æ­¢æ›è¡Œç¬¦]ï¼šåš´ç¦ä½¿ç”¨ \\parã€\\\\ æˆ– \[...\]ã€‚æ‰€æœ‰æ•¸å­¸å¼å¿…é ˆä½¿ç”¨ $...$ (Inline Math)ã€‚
- [è®Šæ•¸æ³¨å…¥]ï¼šå¿…é ˆä½¿ç”¨ r"æ¨¡æ¿".replace("{a}", str(a)) èªæ³•ï¼Œåš´ç¦ç›´æ¥ä½¿ç”¨ f-string è™•ç† LaTeX å€å¡Šã€‚

4. è¦–è¦ºåŒ–å·¥å…·è¦ç¯„ (Visuals)
- [æ•¸ç·šå·¥å…·]ï¼šè‹¥ç‚ºæ•¸ç·šé¡Œï¼Œå¿…é ˆå¯¦ä½œ draw_number_line(points_map) ä¸”è©²å‡½å¼ã€Œæœ€å¾Œå¿…é ˆæœ‰ return html_stringã€ã€‚
- [æ‹¼æ¥è¦æ±‚]ï¼šquestion_text å¿…é ˆç”±ã€Œæ–‡å­—é¡Œç›® + <br> + è¦–è¦ºåŒ– HTMLã€çµ„æˆã€‚

5. æ•¸æ“šèˆ‡æ¬„ä½ (Standard Fields)
- [æ¬„ä½é–æ­»]ï¼šè¿”å›å­—å…¸å¿…é ˆä¸”åƒ…èƒ½åŒ…å« question_text, correct_answer, answer, image_base64ã€‚
- [æ™‚é–“æˆ³è¨˜]ï¼šæ›´æ–°æ™‚å¿…é ˆå°‡ created_at è¨­ç‚º datetime.now() ä¸¦éå¢ versionã€‚
"""

    user_prompt = f"### SKILL: {skill.skill_ch_name} ({skill.skill_id})\n### STRATEGY: {tier_scope}\n### EXECUTE:"
    
    now = datetime.now() # [æ–°å¢] æ•æ‰ç•¶å‰æ™‚é–“

    try:
        client = get_ai_client(role='architect')
        
        # [Fix] å¼·è¡Œè¦†è“‹é è¨­å€¼ (User Request)
        if model_tag == 'cloud_pro':
            architect_model = Config.GEMINI_MODEL_NAME
        elif model_tag == 'local_14b':
            architect_model = Config.LOCAL_MODEL_NAME

        try:
            response = client.generate_content(
                system_instruction + "\n" + user_prompt,
                generation_config={"response_mime_type": "application/json"}
            )
        except:
            response = client.generate_content(system_instruction + "\n" + user_prompt)
            
        response_text = response.text.strip()
        
        # --- JSON è§£æèˆ‡å®¹éŒ¯ (V9.3 Reinforced) ---
        clean_json = response_text.strip()
        # 1. Try to extract strictly from ```json blocks
        block_match = re.search(r'```json\s*(.*?)\s*```', clean_json, re.DOTALL)
        if block_match:
            clean_json = block_match.group(1)
        else:
            # 2. Fallback: Strip Markdown tags if they frame the entire content
            clean_json = re.sub(r'^```json\s*|```$', '', clean_json, flags=re.MULTILINE).strip()
            
        data = {}
        try:
            data = json.loads(clean_json)
        except json.JSONDecodeError:
            print(f"   âš ï¸ JSON Standard Parse Failed. Trying AST...")
            try:
                data = ast.literal_eval(clean_json)
            except:
                print(f"   ğŸš¨ Parsing FAILED. Fallback to Raw.")
                data = {"coder_spec": clean_json, "tutor_guide": "Parsing Failed."}

        # Stringify
        coder_spec = data.get('coder_spec', '')
        if isinstance(coder_spec, (dict, list)): coder_spec = json.dumps(coder_spec, indent=2, ensure_ascii=False)
        else: coder_spec = str(coder_spec)

        tutor_guide = data.get('tutor_guide', '')
        if isinstance(tutor_guide, (dict, list)): tutor_guide = json.dumps(tutor_guide, indent=2, ensure_ascii=False)
        else: tutor_guide = str(tutor_guide)

        # 4. Upsert DB èˆ‡æ™‚é–“æ›´æ–°
        existing_prompt = SkillGenCodePrompt.query.filter_by(skill_id=skill_id, model_tag=model_tag).first()
        
        final_version = 1
        if existing_prompt:
            existing_prompt.user_prompt_template = coder_spec
            existing_prompt.system_prompt = system_instruction
            existing_prompt.version += 1
            existing_prompt.created_at = now # [é—œéµä¿®æ­£] æ›´æ–°æ™‚é–“æˆ³è¨˜ï¼Œè§£æ±ºè³‡æ–™åº«ä¸è·³å‹•å•é¡Œ
            final_version = existing_prompt.version
            print(f"   ğŸ”„ [Upsert] Updated existing prompt (Ver: {final_version})")
        else:
            new_prompt = SkillGenCodePrompt(
                skill_id=skill_id, 
                model_tag=model_tag, 
                user_prompt_template=coder_spec, 
                system_prompt=system_instruction, 
                version=1, 
                is_active=True, 
                architect_model=architect_model,
                created_at=now # [æ–°å¢] åˆå§‹æ™‚é–“
            )
            db.session.add(new_prompt)
            print(f"   ğŸ†• [Upsert] Inserted new prompt entry.")

        if model_tag == 'cloud_pro':
            skill.gemini_prompt = tutor_guide
            print("   ğŸ“¢ [Tutor Guide] Updated (TC).")
        else:
            print(f"   ğŸ”’ [Tutor Guide] Locked.")
        
        db.session.commit()
        return {'success': True, 'version': final_version}

    except Exception as e:
        db.session.rollback()
        print(f"âŒ Error in generate_v9_spec: {str(e)}")
        return {'success': False, 'message': str(e)}