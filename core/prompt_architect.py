# -*- coding: utf-8 -*-
"""
=============================================================================
æ¨¡çµ„åç¨± (Module Name): core/prompt_architect.py
åŠŸèƒ½èªªæ˜ (Description): AI æ¶æ§‹å¸«æ¨¡çµ„ (Architect Mode)ï¼Œè² è²¬åˆ†ææ•™ç§‘æ›¸ä¾‹é¡Œèˆ‡æŠ€èƒ½éœ€æ±‚ï¼Œè¨­è¨ˆä¸¦ç”Ÿæˆçµ¦ Coder AI ä½¿ç”¨çš„è©³ç´° Python å¯¦ä½œè¦æ ¼æ›¸ (Spec)ã€‚
åŸ·è¡Œèªæ³• (Usage): ç”±ç³»çµ±èª¿ç”¨
ç‰ˆæœ¬è³‡è¨Š (Version): V9.3 (Elite Hardening + Timestamp Fix)
æ›´æ–°æ—¥æœŸ (Date): 2026-01-13
ç¶­è­·åœ˜éšŠ (Maintainer): Math AI Project Team
=============================================================================
"""
# ==============================================================================

import json, re, ast
from datetime import datetime # [ä¿®æ­£] å¿…é ˆå°å…¥ datetime
from models import db, SkillInfo, TextbookExample, SkillGenCodePrompt
from core.ai_wrapper import get_ai_client
from config import Config

def generate_v9_spec(skill_id, model_tag='cloud_pro', prompt_strategy='standard', architect_model='human'):
    print(f"--- [Architect v9.3] Analyzing {skill_id} for '{model_tag}' (Elite Mode) ---")
    skill = SkillInfo.query.filter_by(skill_id=skill_id).first()
    if not skill: return {'success': False, 'message': 'Skill not found'}

    # 1. æŠ“å–å…¨é‡ä¾‹é¡Œ
    all_examples = TextbookExample.query.filter_by(skill_id=skill_id).order_by(TextbookExample.id).all()
    if not all_examples: return {'success': False, 'message': 'No examples found'}
    selected_examples = all_examples[:12]
    rag_text = "".join([f"Example {i+1}:\nQ: {getattr(ex, 'problem_text', 'N/A')}\nA: {getattr(ex, 'correct_answer', 'N/A')}\n\n" for i, ex in enumerate(selected_examples)])

    # 2. å®šç¾©åˆ†ç´šç­–ç•¥
    if model_tag == 'edge_7b':
        tier_scope = "Consolidate all examples into ONE single, highly representative function. Keep logic flat and simple."
    elif model_tag == 'local_14b':
        tier_scope = "Consolidate examples into MAX 3 distinct problem types (e.g., Calculation, Concept, Application)."
    else: # cloud_pro
        tier_scope = "Create a rich variety of problem types covering all nuances of the examples."

    # 3. ç³»çµ±æŒ‡ä»¤ (V11.8 é¡å°„å¢å¼·ç‰ˆ)
    system_instruction = """ã€ä»»å‹™ã€‘ï¼šæ“”ä»» K12 æ•¸å­¸ AI é¦–å¸­ç³»çµ±æ¶æ§‹å¸« (V11.8 é¡å°„å¢å¼·ç‰ˆ)

    ä½ çš„ Spec ç”¢å‡ºå¿…é ˆéµå¾ªï¼š
    1. [ç¦çµ•åŸå‰µ]ï¼šä¸è¦è¨­è¨ˆæ–°çš„é¡Œç›®ã€‚è«‹æŒ‡ç¤º Coder å¦‚ä½•éš¨æ©ŸåŒ– RAG ä¸­çš„ç¾æœ‰é¡Œç›®ã€‚
    2. [åº§æ¨™é–æ­»]ï¼šé‡å°å¹¾ä½•é¡Œï¼ŒæŒ‡ç¤º Coder å¿…é ˆæ ¹æ“š RAG åœ–å½¢ï¼ˆå¦‚é•·æ–¹å½¢ ACEFï¼‰å®šç¾©æ­£ç¢ºçš„é ‚é»åº§æ¨™ã€‚

    3. ç¨‹å¼çµæ§‹ (Structure Hardening)
    - [é ‚å±¤å‡½å¼]ï¼šåš´ç¦ä½¿ç”¨ class å°è£ã€‚å¿…é ˆç›´æ¥å®šç¾© generate(level=1) èˆ‡ check(user_answer, correct_answer) æ–¼æ¨¡çµ„æœ€å¤–å±¤ã€‚
    - [è‡ªå‹•é‡è¼‰]ï¼šç¢ºä¿ä»£ç¢¼ä¸ä¾è³´å…¨åŸŸç‹€æ…‹ï¼Œä»¥ä¾¿ç³»çµ±åŸ·è¡Œ importlib.reloadã€‚

    4. é¡Œå‹é¡å°„ (Problem Mirroring)
    - [éš¨æ©Ÿåˆ†æµ]ï¼šgenerate() å…§éƒ¨å¿…é ˆä½¿ç”¨ random.choice æˆ– if/elif é‚è¼¯ï¼Œæ˜ç¢ºå°æ‡‰åˆ° RAG ä¸­çš„ä¾‹é¡Œ (Type 1 -> Ex 1)ã€‚
    - [ç¯„ä¾‹]ï¼šSpec å¿…é ˆæè¿°å¦‚ä½•å°‡ RAG ä¾‹é¡Œä¸­çš„æ•¸æ“šã€Œå‹•æ…‹åŒ–ã€(Dynamize)ï¼Œè€Œä¸æ˜¯å‰µé€ æ–°é¡Œå‹ã€‚

    5. æ’ç‰ˆèˆ‡ LaTeX å®‰å…¨ (Elite Guardrails)
    - ã€å¼·åˆ¶ã€‘èªæ³•é›¶ä¿®å¾© (Regex=0)ï¼š
      å‡¡å­—ä¸²åŒ…å« LaTeX æŒ‡ä»¤ (å¦‚ \\frac, \\sqrt, \\pm)ï¼Œåš´ç¦ä½¿ç”¨ f-string æˆ– % æ ¼å¼åŒ–ã€‚
      å¿…é ˆåš´æ ¼åŸ·è¡Œä»¥ä¸‹æ¨¡æ¿ï¼š
      ans_val = 5
      expr = r"x = {a}".replace("{a}", str(ans_val))
      
    - ã€åš´ç¦ã€‘ä¸å¯ä½¿ç”¨ f"x = {ans_val}"ï¼Œå› ç‚ºé€™æœƒå°è‡´ LaTeX çš„å¤§æ‹¬è™Ÿèˆ‡ Python è¡çªã€‚
    - ã€æ’ç‰ˆã€‘åš´ç¦ä½¿ç”¨ \\par æˆ– \\[...\\]ã€‚æ‰€æœ‰æ•¸å­¸å¼ä¸€å¾‹ä½¿ç”¨ $...$ã€‚


    6. è¦–è¦ºåŒ–èˆ‡è¼”åŠ©å‡½å¼é€šç”¨è¦ç¯„ (Generic Helper Rules)
    - [å¿…é ˆå›å‚³]ï¼šæ‰€æœ‰å®šç¾©çš„è¼”åŠ©å‡½å¼ï¼ˆå¦‚ draw_ é–‹é ­æˆ–è‡ªå®šç¾©é‹ç®—å‡½å¼ï¼‰ï¼Œæœ€å¾Œä¸€è¡Œå¿…é ˆæ˜ç¢ºä½¿ç”¨ 'return' èªå¥å›å‚³çµæœã€‚
    - [é¡å‹ä¸€è‡´]ï¼šè‹¥è©²å‡½å¼çµæœæœƒç”¨æ–¼æ‹¼æ¥ question_textï¼Œå‰‡å›å‚³å€¼å¿…é ˆå¼·åˆ¶è½‰ç‚ºå­—ä¸² (str)ã€‚
    - [é˜²æ´©æ¼åŸå‰‡]ï¼šè¦–è¦ºåŒ–å‡½å¼åƒ…èƒ½æ¥æ”¶ã€Œé¡Œç›®å·²çŸ¥æ•¸æ“šã€ã€‚åš´ç¦å°‡ã€Œç­”æ¡ˆæ•¸æ“šã€å‚³å…¥ç¹ªåœ–å‡½å¼ï¼Œç¢ºä¿å­¸ç”Ÿç„¡æ³•å¾åœ–å½¢ä¸­ç›´æ¥çœ‹åˆ°ç­”æ¡ˆã€‚

    7. æ•¸æ“šèˆ‡æ¬„ä½ (Standard Fields)
    - [æ¬„ä½é–æ­»]ï¼šè¿”å›å­—å…¸å¿…é ˆä¸”åƒ…èƒ½åŒ…å« question_text, correct_answer, answer, image_base64ã€‚
    - [æ™‚é–“æˆ³è¨˜]ï¼šæ›´æ–°æ™‚å¿…é ˆå°‡ created_at è¨­ç‚º datetime.now() ä¸¦éå¢ versionã€‚

    8. ç‰¹æ®Šé ˜åŸŸä¿è­· (Domain Specific Rules)
    - [çŸ©é™£èˆ‡è¡Œåˆ—å¼]ï¼šè‹¥æŠ€èƒ½æ¶‰åŠ Matrix æˆ– Determinantï¼š
      - correct_answer å¿…é ˆç‚ºå­—ä¸²åŒ–çš„äºŒç¶­åˆ—è¡¨ (e.g., "[[1,2],[3,4]]")ã€‚
      - å¿…é ˆå¼·åˆ¶è§¸ç™¼æ‰‹å¯«æ¨¡å¼ (åœ¨ question_text åŒ…å« "^" æˆ– "[" ç­‰æ‰‹å¯«ç‰¹å¾µç¬¦è™Ÿ)ã€‚

    9. é¡Œç›®å°æ‡‰ (Problem Type Mapping)
    - [å°æ‡‰æ©Ÿåˆ¶]ï¼šåœ¨ Spec ä¸­å®šç¾©æ¯å€‹ Problem Type æ™‚ï¼Œå¿…é ˆæ˜ç¢ºæŒ‡å‡ºå…¶å°æ‡‰çš„è³‡æ–™åº«ä¾‹é¡Œç·¨è™Ÿã€‚
    - [æ ¼å¼]ï¼šä¾‹å¦‚ "Type 1 (Maps to Example 1, 3): Description..."ã€‚ç¢ºä¿è¨­è¨ˆçš„é‚è¼¯ç·Šå¯†è·Ÿéš¨æ•™ç§‘æ›¸ç¯„ä¾‹ã€‚

    10. æ•¸æ“šç¦çµ•å¸¸æ•¸ (Data Prohibition) [CRITICAL]
    - [éš¨æ©Ÿç”Ÿæˆ]ï¼šSpec å¿…é ˆæ˜ç¢ºè¦æ±‚ Coder ä½¿ç”¨ random.randint ç”Ÿæˆæ‰€æœ‰å¹¾ä½•é•·åº¦ã€è§’åº¦èˆ‡é¢ç©ã€‚
    - [å…¬å¼è¨ˆç®—]ï¼šåš´ç¦ç¡¬ç·¨ç¢¼ (Hardcode) ç­”æ¡ˆæˆ–åº§æ¨™ã€‚æ‰€æœ‰ç›®æ¨™ç­”æ¡ˆèˆ‡åœ–å½¢åº§æ¨™å¿…é ˆæ ¹æ“šéš¨æ©Ÿç”Ÿæˆçš„æ•¸æ“šï¼Œé€éå¹¾ä½•å…¬å¼åå‘è¨ˆç®—å¾—å‡ºã€‚

    """

    user_prompt = f"### SKILL: {skill.skill_ch_name} ({skill.skill_id})\n### STRATEGY: {tier_scope}\n### EXECUTE:"
    
    now = datetime.now() # [æ–°å¢] æ•æ‰ç•¶å‰æ™‚é–“

    try:
        client = get_ai_client(role='architect')
        
        # [Fix] å¼·è¡Œè¦†è“‹é è¨­å€¼ (User Request)
        if model_tag == 'cloud_pro':
            architect_model = Config.GEMINI_MODEL_NAME
        elif model_tag == 'local_14b':
            architect_model = Config.LOCAL_MODEL_NAME

        try:
            response = client.generate_content(
                system_instruction + "\n" + user_prompt,
                generation_config={"response_mime_type": "application/json"}
            )
        except:
            response = client.generate_content(system_instruction + "\n" + user_prompt)
            
        response_text = response.text.strip()
        
        # --- JSON è§£æèˆ‡å®¹éŒ¯ (V9.3 Reinforced) ---
        clean_json = response_text.strip()
        # 1. Try to extract strictly from ```json blocks
        block_match = re.search(r'```json\s*(.*?)\s*```', clean_json, re.DOTALL)
        if block_match:
            clean_json = block_match.group(1)
        else:
            # 2. Fallback: Strip Markdown tags if they frame the entire content
            clean_json = re.sub(r'^```json\s*|```$', '', clean_json, flags=re.MULTILINE).strip()
            
        data = {}
        try:
            data = json.loads(clean_json)
        except json.JSONDecodeError:
            print(f"   âš ï¸ JSON Standard Parse Failed. Trying AST...")
            try:
                data = ast.literal_eval(clean_json)
            except:
                print(f"   ğŸš¨ Parsing FAILED. Fallback to Raw.")
                data = {"coder_spec": clean_json, "tutor_guide": "Parsing Failed."}

        # Stringify
        coder_spec = data.get('coder_spec', '')
        if isinstance(coder_spec, (dict, list)): coder_spec = json.dumps(coder_spec, indent=2, ensure_ascii=False)
        else: coder_spec = str(coder_spec)

        tutor_guide = data.get('tutor_guide', '')
        if isinstance(tutor_guide, (dict, list)): tutor_guide = json.dumps(tutor_guide, indent=2, ensure_ascii=False)
        else: tutor_guide = str(tutor_guide)

        # 4. Upsert DB èˆ‡æ™‚é–“æ›´æ–°
        existing_prompt = SkillGenCodePrompt.query.filter_by(skill_id=skill_id, model_tag=model_tag).first()
        
        final_version = 1
        if existing_prompt:
            existing_prompt.user_prompt_template = coder_spec
            existing_prompt.system_prompt = system_instruction
            existing_prompt.version += 1
            existing_prompt.created_at = now # [é—œéµä¿®æ­£] æ›´æ–°æ™‚é–“æˆ³è¨˜ï¼Œè§£æ±ºè³‡æ–™åº«ä¸è·³å‹•å•é¡Œ
            final_version = existing_prompt.version
            print(f"   ğŸ”„ [Upsert] Updated existing prompt (Ver: {final_version})")
        else:
            new_prompt = SkillGenCodePrompt(
                skill_id=skill_id, 
                model_tag=model_tag, 
                user_prompt_template=coder_spec, 
                system_prompt=system_instruction, 
                version=1, 
                is_active=True, 
                architect_model=architect_model,
                created_at=now # [æ–°å¢] åˆå§‹æ™‚é–“
            )
            db.session.add(new_prompt)
            print(f"   ğŸ†• [Upsert] Inserted new prompt entry.")

        if model_tag == 'cloud_pro':
            skill.gemini_prompt = tutor_guide
            print("   ğŸ“¢ [Tutor Guide] Updated (TC).")
        else:
            print(f"   ğŸ”’ [Tutor Guide] Locked.")
        
        db.session.commit()
        return {'success': True, 'version': final_version}

    except Exception as e:
        db.session.rollback()
        print(f"âŒ Error in generate_v9_spec: {str(e)}")
        return {'success': False, 'message': str(e)}