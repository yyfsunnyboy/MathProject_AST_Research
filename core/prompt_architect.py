# -*- coding: utf-8 -*-
"""
=============================================================================
æ¨¡çµ„åç¨± (Module Name): core/prompt_architect.py
åŠŸèƒ½èªªæ˜ (Description): 
    V15 Architect (Hardening Edition)
    è² è²¬ç”¢å‡ºå…·å‚™ 6 ç¨®æ¨¡å¼ã€LaTeX è¦ç´„èˆ‡é‚è¼¯çŸ©é™£çš„æ•¸å­¸æŠ€èƒ½è¦æ ¼ (Spec)ã€‚
    æ­¤æ¨¡çµ„æ˜¯ "Prompt Engineering" çš„æ ¸å¿ƒï¼Œè² è²¬æŒ‡æ® Coder å¦‚ä½•æ’°å¯«ç¨‹å¼ç¢¼ã€‚
    
ç‰ˆæœ¬è³‡è¨Š (Version): V15.0
æ›´æ–°æ—¥æœŸ (Date): 2026-01-18
ç¶­è­·åœ˜éšŠ (Maintainer): Math AI Project Team
=============================================================================
"""

import os
import json
import re
import time
from datetime import datetime
from flask import current_app
from models import db, SkillInfo, SkillGenCodePrompt, TextbookExample
from core.ai_wrapper import get_ai_client

# ==============================================================================
# V15 SYSTEM PROMPT (The "Blueprints")
# ==============================================================================
# ==============================================================================
# V15.1 HYBRID SYSTEM PROMPT
# ==============================================================================
V15_1_SYSTEM_PROMPT = """Role: Senior Mathematics Curriculum Architect (Taiwan).

### â›” MISSION:
Analyze the RAG examples and design a "Master Coding Spec" for random question generation.

### ğŸŒ LANGUAGE RULES:
1. ALL output text MUST be in Traditional Chinese (ç¹é«”ä¸­æ–‡, å°ç£ç”¨èª).
2. USE local terms: e.g., "è¨ˆç®—ä¸‹åˆ—å„å¼çš„å€¼", "æœ€ç°¡åˆ†æ•¸", "åˆ†é…å¾‹".

### ğŸ§© LOGIC MATRIX (3x2 Strategy):
- Extract the core math logic (integers, fractions, brackets).
- Mode 1-2: Basic arithmetic.
- Mode 3-4: Intermediate (Nested brackets or absolute values).
- Mode 5-6: Advanced (Distributive law or multi-step logic).

### ğŸ§ª OUTPUT FORMAT:
- Variable naming: You MUST instruct the Coder to use 'q' for question text and 'a' for the answer string.
- No Context: If the examples are pure math, do NOT force scenarios like "deposits" or "temperature".
"""

def generate_v15_spec(skill_id, model_tag="cloud_pro", architect_model=None):
    """
    [V15.1 Hybrid Architect] 
    ä½¿ç”¨æ··åˆèªè¨€ç­–ç•¥ï¼šè‹±æ–‡å®šç¾©é‚è¼¯çµæ§‹ï¼Œä¸­æ–‡å®šç¾©æƒ…å¢ƒå…§å®¹ã€‚
    Adaption: Uses core.ai_wrapper for compatibility.
    """
    try:
        # 1. Fetch Data
        skill = SkillInfo.query.filter_by(skill_id=skill_id).first()
        if not skill:
            return {'success': False, 'message': f"Skill {skill_id} not found."}

        # RAG: Get textbook examples
        examples = TextbookExample.query.filter_by(skill_id=skill_id).limit(3).all()
        examples_text = []
        if examples:
            examples_text = [f"{e.problem_text} (Sol: {e.detailed_solution})" for e in examples]
        else:
            examples_text = ["(No textbook examples found. Base design on skill name.)"]

        # 2. Build Prompt (Hybrid Strategy)
        # Using concatenation since ai_wrapper supports single prompt argument
        rag_block = chr(10).join([f"Example {i+1}: {ex}" for i, ex in enumerate(examples_text)])
        
        user_prompt = f"""Skill ID: {skill_id}
Skill Name: {skill.skill_ch_name}

### RAG EXAMPLES (Reference Material):
{rag_block}

### TASK:
Analyze the 3 examples above. 
1. Extract their scenarios into `SCENARIO_DB`. 
2. Define the 3x2 Mirror Logic Matrix. 
3. Generate the final coding specification in a rigorous, logical format.
"""
        
        full_prompt = V15_1_SYSTEM_PROMPT + "\n\n" + user_prompt

        # 3. Call AI
        client = get_ai_client(role='architect') 
        
        print(f"   ğŸ§  V15.1 Architect is thinking... (Skill: {skill.skill_ch_name})")
        # Note: ai_wrapper handles api keys and model selection based on config
        response = client.generate_content(full_prompt)
        spec_content = response.text
        
        if not spec_content:
            return {'success': False, 'message': "Empty response from AI."}

        # 4. Save to Database (MASTER_SPEC Strategy)
        skill.gemini_prompt = spec_content
        
        # æ°¸é ä»¥ MASTER_SPEC æ¨™ç±¤å­˜æª”ï¼Œè¦†è“‹æˆ–æ–°å¢éƒ½ä¸å½±éŸ¿è®€å–
        new_prompt_entry = SkillGenCodePrompt(
            skill_id=skill_id,
            prompt_content=spec_content, # çµ±ä¸€ä½¿ç”¨ prompt_content
            prompt_type="MASTER_SPEC",    # å›ºå®šæ¨™ç±¤ï¼Œä¸å†æ›´å‹•ç‰ˆæœ¬è™Ÿ
            system_prompt=V15_1_SYSTEM_PROMPT, 
            user_prompt_template=user_prompt,
            model_tag=model_tag,          # Keep tag mostly for debugging/logging origin
            architect_model=architect_model or "default_architect",
            created_at=datetime.now()
        )
        db.session.add(new_prompt_entry)
        db.session.commit()

        return {
            'success': True,
            'version': 15.1,
            'spec': spec_content, 
            'message': "V15.1 Spec generated successfully."
        }

    except Exception as e:
        db.session.rollback()
        return {'success': False, 'message': str(e)}

# Alias for backward compatibility if needed (though we updated callers)
# generate_v9_spec = generate_v15_spec 