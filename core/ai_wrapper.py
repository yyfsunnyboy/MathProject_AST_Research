import requests
import google.generativeai as genai
from flask import current_app
# å¾æˆ‘å€‘å‰›å‰›æ”¹å¥½çš„ config.py åŒ¯å…¥è¨­å®š
from config import Config 

class AIResponse:
    """
    æ¨™æº–åŒ–å›æ‡‰ç‰©ä»¶ï¼š
    ç„¡è«–æ˜¯ Gemini é‚„æ˜¯ Local AIï¼Œå›å‚³çµ¦ä¸»ç¨‹å¼çš„çµæœ
    éƒ½æœƒè¢«åŒ…è£æˆé€™å€‹ç‰©ä»¶ï¼Œçµ±ä¸€é€é .text å±¬æ€§å–å¾—å…§å®¹ã€‚
    """
    def __init__(self, text):
        self.text = text

class GeminiClient:
    """
    é›²ç«¯é©é…å™¨ï¼šè² è²¬è·Ÿ Google Gemini æºé€š
    """
    def __init__(self):
        if not Config.GEMINI_API_KEY:
            error_msg = "âŒ éŒ¯èª¤ï¼šGemini æ¨¡å¼éœ€è¦è¨­å®š GEMINI_API_KEY"
            if current_app: current_app.logger.error(error_msg)
            raise ValueError(error_msg)
            
        genai.configure(api_key=Config.GEMINI_API_KEY)
        self.model = genai.GenerativeModel(Config.GEMINI_MODEL_NAME)

    def generate_content(self, prompt):
        try:
            # Google SDK çš„æ¨™æº–å‘¼å«æ–¹å¼
            response = self.model.generate_content(prompt)
            return AIResponse(response.text)
        except Exception as e:
            error_msg = f"Gemini API Error: {str(e)}"
            if current_app: current_app.logger.error(error_msg)
            return AIResponse(error_msg)

class LocalLLMClient:
    """
    æœ¬åœ°é©é…å™¨ï¼šè² è²¬è·Ÿ Ollama (DeepSeek) æºé€š
    é€™å°±æ˜¯æˆ‘å€‘ç§‘å±•å¯¦é©—çš„æ ¸å¿ƒï¼
    """
    def __init__(self):
        self.api_url = Config.LOCAL_API_URL
        self.model_name = Config.LOCAL_MODEL_NAME

    def generate_content(self, prompt):
        # Ollama API çš„æ¨™æº–æ ¼å¼
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False, # ä¸ä½¿ç”¨ä¸²æµï¼Œä¸€æ¬¡æ‹¿å›å®Œæ•´çµæœ
            "options": {
                "temperature": 0.2, # ä½æº«æ¨¡å¼ï¼Œè®“å¯«ç¨‹å¼æ›´ç²¾ç¢ºã€é‚è¼¯æ›´åš´è¬¹
                "num_ctx": 4096     # ç¢ºä¿å®ƒèƒ½è®€å®Œæˆ‘å€‘é•·é•·çš„ 13 é» Prompt
            }
        }
        try:
            # ç™¼é€è«‹æ±‚çµ¦æœ¬æ©Ÿçš„ Ollama
            response = requests.post(self.api_url, json=payload)
            response.raise_for_status()
            
            # å¾ Ollama çš„å›å‚³ JSON ä¸­æå–æ–‡å­—
            result_text = response.json().get("response", "")
            return AIResponse(result_text)
            
        except Exception as e:
            error_msg = f"Local AI (Ollama) Error: {str(e)}\nè«‹ç¢ºèª Ollama æ‡‰ç”¨ç¨‹å¼æ˜¯å¦å·²å•Ÿå‹•ï¼Ÿ"
            if current_app: current_app.logger.error(error_msg)
            return AIResponse(error_msg)

def get_ai_client():
    """
    å·¥å» å‡½å¼ (Factory Function)ï¼š
    æ ¹æ“š config.py çš„ AI_PROVIDER é–‹é—œï¼Œæ±ºå®šç¾åœ¨è¦æ´¾èª°ä¸Šå ´ã€‚
    """
    # è½‰å°å¯«ä»¥é˜²è¬ä¸€
    provider = Config.AI_PROVIDER.lower()
    
    if provider == 'gemini':
        if current_app: 
            current_app.logger.info(f"âœ¨ [AI Mode] Google Gemini ({Config.GEMINI_MODEL_NAME})")
        return GeminiClient()
        
    elif provider == 'local':
        if current_app: 
            current_app.logger.info(f"ğŸ’» [AI Mode] Local Ollama ({Config.LOCAL_MODEL_NAME})")
        return LocalLLMClient()
        
    else:
        if current_app:
            current_app.logger.warning(f"âš ï¸ æœªçŸ¥çš„ AI_PROVIDER: {provider}ï¼Œå¼·åˆ¶åˆ‡æ›è‡³ Local æ¨¡å¼")
        return LocalLLMClient()