import requests
import google.generativeai as genai
from flask import current_app
# å¾æˆ‘å€‘å‰›å‰›æ”¹å¥½çš„ config.py åŒ¯å…¥è¨­å®š
from config import Config 

class AIResponse:
    """
    æ¨™æº–åŒ–å›æ‡‰ç‰©ä»¶ï¼š
    ç„¡è«–æ˜¯ Gemini é‚„æ˜¯ Local AIï¼Œå›å‚³çµ¦ä¸»ç¨‹å¼çš„çµæœ
    éƒ½æœƒè¢«åŒ…è£æˆé€™å€‹ç‰©ä»¶ï¼Œçµ±ä¸€é€é .text å±¬æ€§å–å¾—å…§å®¹ã€‚
    """
    def __init__(self, text):
        self.text = text

class GeminiClient:
    """
    é›²ç«¯é©é…å™¨ï¼šè² è²¬è·Ÿ Google Gemini æºé€š
    """
    def __init__(self, model_name=None):
        if not Config.GEMINI_API_KEY:
            error_msg = "âŒ éŒ¯èª¤ï¼šGemini æ¨¡å¼éœ€è¦è¨­å®š GEMINI_API_KEY"
            if current_app: current_app.logger.error(error_msg)
            raise ValueError(error_msg)
            
        genai.configure(api_key=Config.GEMINI_API_KEY)
        # Use provided model_name or fallback to default
        self.model_name = model_name if model_name else "gemini-1.5-flash"
        self.model = genai.GenerativeModel(self.model_name)

    def generate_content(self, prompt):
        try:
            # Google SDK çš„æ¨™æº–å‘¼å«æ–¹å¼
            response = self.model.generate_content(prompt)
            return AIResponse(response.text)
        except Exception as e:
            error_msg = f"Gemini API Error: {str(e)}"
            if current_app: current_app.logger.error(error_msg)
            return AIResponse(error_msg)

class LocalLLMClient:
    """
    æœ¬åœ°é©é…å™¨ï¼šè² è²¬è·Ÿ Ollama (DeepSeek) æºé€š
    é€™å°±æ˜¯æˆ‘å€‘ç§‘å±•å¯¦é©—çš„æ ¸å¿ƒï¼
    """
    def __init__(self, model_name=None):
        self.api_url = Config.LOCAL_API_URL
        # Use provided model_name or fallback to default
        self.model_name = model_name if model_name else "qwen2.5-coder:3b"

    def generate_content(self, prompt):
        # Ollama API çš„æ¨™æº–æ ¼å¼
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False, # ä¸ä½¿ç”¨ä¸²æµï¼Œä¸€æ¬¡æ‹¿å›å®Œæ•´çµæœ
            "options": {
                "temperature": 0.2, # ä½æº«æ¨¡å¼ï¼Œè®“å¯«ç¨‹å¼æ›´ç²¾ç¢ºã€é‚è¼¯æ›´åš´è¬¹
                "num_ctx": 4096     # ç¢ºä¿å®ƒèƒ½è®€å®Œæˆ‘å€‘é•·é•·çš„ 13 é» Prompt
            }
        }
        try:
            # ç™¼é€è«‹æ±‚çµ¦æœ¬æ©Ÿçš„ Ollama
            response = requests.post(self.api_url, json=payload)
            response.raise_for_status()
            
            # å¾ Ollama çš„å›å‚³ JSON ä¸­æå–æ–‡å­—
            result_text = response.json().get("response", "")
            return AIResponse(result_text)
            
        except Exception as e:
            error_msg = f"Local AI (Ollama) Error: {str(e)}\nè«‹ç¢ºèª Ollama æ‡‰ç”¨ç¨‹å¼æ˜¯å¦å·²å•Ÿå‹•ï¼Ÿ"
            if current_app: current_app.logger.error(error_msg)
            return AIResponse(error_msg)

def get_ai_client(role='default'):
    """
    å·¥å» å‡½å¼ (Factory Function)ï¼š
    æ ¹æ“š role åƒæ•¸å¾ Config.MODEL_ROLES å–å¾—å°æ‡‰çš„ provider èˆ‡ modelã€‚
    """
    # 1. å–å¾—è§’è‰²è¨­å®šï¼Œè‹¥æ‰¾ä¸åˆ°å‰‡å›é€€åˆ° default
    role_config = Config.MODEL_ROLES.get(role, Config.MODEL_ROLES.get('default'))
    
    # 2. è§£æè¨­å®š
    provider = role_config.get('provider', 'local').lower()
    model_name = role_config.get('model')
    
    # 3. åˆ†æ´¾å®¢æˆ¶ç«¯
    if provider == 'gemini':
        if current_app: 
            current_app.logger.info(f"âœ¨ [AI Mode] Role: {role} -> Google Gemini ({model_name})")
        return GeminiClient(model_name)
        
    elif provider == 'local':
        if current_app: 
            current_app.logger.info(f"ğŸ’» [AI Mode] Role: {role} -> Local Ollama ({model_name})")
        return LocalLLMClient(model_name)
        
    else:
        if current_app:
            current_app.logger.warning(f"âš ï¸ æœªçŸ¥çš„ Provider: {provider}ï¼Œå¼·åˆ¶åˆ‡æ›è‡³ Local æ¨¡å¼")
        return LocalLLMClient(model_name)