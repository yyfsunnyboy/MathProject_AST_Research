def auto_generate_skill_code(skill_id, queue=None, **kwargs):
    """
    æ›´æ–°å¾Œçš„ç”Ÿæˆå‡½å¼ï¼Œæ”¯æ´ 3x3 å¯¦é©—æ•¸æ“šæ¡é›†ã€‚
    """
    start_time = time.time()
    
    # 1. Determine Target Tag based on Config
    role_config = Config.MODEL_ROLES.get('coder', Config.MODEL_ROLES.get('default'))
    current_model = role_config.get('model', 'Unknown')
    current_provider = role_config.get('provider', 'Unknown') # æŠ“å–å¯¦éš› provider
    target_tag = infer_model_tag(current_model)
    
    # [ç§‘ç ”åƒæ•¸æå–] å¾ kwargs å–å¾—å¯¦é©—åƒæ•¸ï¼Œè‹¥ç„¡å‰‡çµ¦é è¨­å€¼
    ablation_id = kwargs.get('ablation_id', 1) # é è¨­ç‚º Bare
    model_size_class = kwargs.get('model_size_class', 'Cloud')
    prompt_level = kwargs.get('prompt_level', 'Bare')

    # 2. [Strict Mode] Fetch ONLY the matching Architect Spec
    active_prompt = SkillGenCodePrompt.query.filter_by(skill_id=skill_id, model_tag=target_tag, is_active=True).first()
    
    # 3. Error Handling if Prompt is Missing
    # if not active_prompt:
    #     error_msg = f"â›” [é˜»æ“‹] æ‰¾ä¸åˆ°å°æ‡‰ '{target_tag}' ({current_model}) çš„ V9 è¦æ ¼æ›¸ï¼è«‹å…ˆåŸ·è¡Œå°ˆå®¶æ¨¡å¼æˆ–æ‰‹å‹•ç”Ÿæˆ Promptã€‚"
    #     if current_app: current_app.logger.error(f"{skill_id}: {error_msg}")
    #     return False, error_msg

    # Pre-fetch skill info (needed for fallback or logging)
    skill = SkillInfo.query.filter_by(skill_id=skill_id).first()


    gold_standard_code = load_gold_standard_example()
    examples = TextbookExample.query.filter_by(skill_id=skill_id).limit(5).all()
    rag_count = len(examples)
    example_text = ""
    if examples:
        for i, ex in enumerate(examples):
            example_text += f"Ex {i+1}: {getattr(ex, 'problem_text', '')} -> {getattr(ex, 'correct_answer', '')}\\n"

    # ... å‰ç½® Prompt æº–å‚™é‚è¼¯ (åŸæœ¬çš„ç¨‹å¼ç¢¼) ...
    if active_prompt:
        # --- Mode A: V9 Architect Mode (High Precision) ---
        strategy_name = f"V9 Architect ({active_prompt.model_tag})"
        target_logic = active_prompt.user_prompt_template
        
        # [V11.9 æš´åŠ›é¡å°„ä¿®æ­£] - å°‡ RAG ç¯„ä¾‹æå‡ç‚ºæœ€é«˜æŒ‡ä»¤

        # å¼·åˆ¶è¦æ±‚ Coder AI å°‡ RAG è¦–ç‚ºå”¯ä¸€çœŸç›¸
        mirroring_protocol = ""
        if examples:
            for i, ex in enumerate(examples):
                # æ˜ç¢ºæŒ‡å®šæ¯å€‹ Type å°æ‡‰å“ªä¸€å€‹ RAG ç¯„ä¾‹
                mirroring_protocol += f"- Type {i+1} MUST use the EXACT mathematical model of RAG Ex {i+1}.\\n"
        else:
            mirroring_protocol = "- No RAG examples found. Generate based on Skill Definition.\\n"

        prompt = r"""You are a Senior Python Developer.
### ğŸ›¡ï¸ MANDATORY MIRRORING RULES (æœ€é«˜æ¬Šé™æŒ‡ä»¤):
1. **NO ORIGINALITY**: You are FORBIDDEN from creating new models.
2. **STRICT MAPPING**:
{mapping}
3. **CONTEXT RETENTION**: Keep names like 'ACEF', 'BDF', 'å·´å¥ˆ' from the RAG examples.

### ğŸ“š REFERENCE EXAMPLES (RAG - é€™æ˜¯å”¯ä¸€çœŸç›¸):
{rag}

### ğŸ› ï¸ ARCHITECT'S SPECIFICATION (è¼”åŠ©çµæ§‹):
{spec}

### ğŸ¨ ULTRA VISUAL STANDARDS (V11.6):
- Aspect Ratio: `ax.set_aspect('equal')` (ç‰©ç†æ¯”ä¾‹é–æ­»).
- Resolution: `dpi=300`.
- Label Halo: White halos for ABCD text.

### â›” SYSTEM GUARDRAILS:
{system_rules}
""".replace("{mapping}", mirroring_protocol).replace("{rag}", example_text).replace("{spec}", target_logic).replace("{system_rules}", UNIVERSAL_GEN_CODE_PROMPT)
    else:
        # --- Mode B: Legacy V8 Mode (Fallback) ---
        strategy_name = "Standard Mode"
        target_logic = skill.gemini_prompt if (skill and skill.gemini_prompt) else f"Math logic for {skill_id}"
        
        # [v11.7 Upgrade]: Prompt Optimization - Pedagogical Mirroring
        prompt = f"""
You are a Senior Python Engineer for a Math Education System.

### MISSION:
Implement the skill `{skill_id}` by strictly following the **Architect's Spec**.

### IMPORTANT: DO NOT WRITE HELPER FUNCTIONS
The system will automatically inject standard helpers (`to_latex`, `fmt_num`, `get_random_fraction`, `is_prime`, etc.) at runtime.
**YOU MUST NOT DEFINE THEM.** Just use them directly.

### REFERENCE STRUCTURE (GOLD STANDARD v3.0):
```python
import random
import math
from fractions import Fraction

# (Helpers are auto-injected here, do not write them)

def generate_type_1_problem():
    val = get_random_fraction()
    # Question needs LaTeX wrapping:
    q = f"What is ${{to_latex(val)}}?"
    # Answer MUST be clean (NO $ signs):
    a = to_latex(val) 
    return {{'question_text': q, 'answer': a, 'correct_answer': a}}

def generate(level=1):
    # Dispatcher logic
    ...
ARCHITECT'S SPECIFICATION: {target_logic}

### REFERENCE EXAMPLES (RAG):
{example_text}

### ğŸ’¡ INSTRUCTION:
Your task is to dynamize (Dynamize) the following examples into Python code, strictly adhering to their mathematical models.

### ğŸ›¡ï¸ PEDAGOGICAL PRIORITY PROTOCOL (V11.7):
1. **Type 1 - Textbook Mirroring (Mirror Mode)**:
   - You MUST generate `generate_type_1` by strictly mirroring the first RAG example.
   - **NO ORIGINALITY**: Use the EXACT same mathematical model. ONLY Randomize the numbers.
   - **Context**: Keep keywords like "Aquarium", "Ticket". Do not change context.

2. **Data Linkage (Integer Guarantee)**:
   - For Reverse Calculation problems, generate the integer ANSWER first, then derive the question parameters.

CODING RULES:

1. **NO HELPERS**: Do NOT define `to_latex`, `fmt_num`, `check`, etc. They are auto-injected. Use them directly.

2. **Smart Dispatcher**: Implement `def generate(level=1):` to handle difficulty levels.
   - **[é‡è¦ï¼šå‡½å¼å‘½åè¦ç¯„]** ä¸è«–é¡Œç›®é¡å‹ç‚ºä½•ï¼Œä¸»ç”Ÿæˆå‡½å¼å¿…é ˆçµ±ä¸€å‘½åç‚º `generate()`ã€‚
   - ç¦æ­¢ä½¿ç”¨ `generate_number_line()` æˆ– `generate_logic()` ç­‰è‡ªå®šç¾©åç¨±ã€‚
   - å¦‚æœæœ‰ç¹ªåœ–è¼”åŠ©å‡½å¼ï¼ˆå¦‚ `draw_graph`ï¼‰ï¼Œè«‹åœ¨ `generate()` å‡½å¼å…§éƒ¨å‘¼å«å®ƒã€‚
   - å¿…é ˆç¢ºä¿æª”æ¡ˆä¸­å­˜åœ¨ `def generate():` å’Œ `def check(user_answer, correct_answer):`ã€‚

3. **LaTeX Formatting (CRITICAL)**: 
   - All mathematical expressions (integers, fractions, equations) in `question_text` MUST be wrapped in single dollar signs `$`.
   - Example: `f"è¨ˆç®— ${fmt_num(a)} + {fmt_num(b)}$ çš„å€¼"` -> "è¨ˆç®— $3 + 5$ çš„å€¼".
   - **NO BACKTICKS**: Never use backticks (`) to wrap numbers or lists. BAD: `[1, 2]`. GOOD: $1, 2$.

4. **Answer Format Hint (CRITICAL)**:
   - You **MUST** append a clear format hint at the very end of `question_text`.
   - Format: `\\n(ç­”æ¡ˆæ ¼å¼ï¼š...)`.
   - Example 1 (Values): `... \\n(ç­”æ¡ˆæ ¼å¼ï¼šè«‹å¡«å¯«æ•´æ•¸)` or `... \\n(ç­”æ¡ˆæ ¼å¼ï¼šæœ€ç°¡åˆ†æ•¸)`.
   - Example 2 (Variables): `... \\n(ç­”æ¡ˆæ ¼å¼ï¼šx=_, y=_)` (This ensures specific ordering).
   - Example 3 (Coordinates): `... \\n(ç­”æ¡ˆæ ¼å¼ï¼š(x,y))`.

5. **Return Keys**: Return dict with keys: `'question_text'`, `'answer'`, `'correct_answer'`.
   - `correct_answer`: Must be a clean string for checking (e.g., "-5", "3/4", "x=2, y=3"). 
   - Do NOT use LaTeX (`$`) in `correct_answer` or `answer` keys, as this makes user input matching difficult. Keep it raw text.

6. **Language**: Traditional Chinese (Taiwan) ONLY (ç¹é«”ä¸­æ–‡). Use local terminology (e.g., åº§æ¨™, è¯ç«‹æ–¹ç¨‹å¼).

7. **Level Completeness**: Implement both Level 1 (Basic) and Level 2 (Advanced/Applied).

OUTPUT: Return ONLY the Python code. Start with `import random`.

[é˜²å‘†è¼¸å‡ºè¦æ±‚] åœ¨ Python æª”æ¡ˆçš„æœ€æœ«å°¾ï¼Œè«‹å‹™å¿…åŒ…å«ä»¥ä¸‹ä»£ç¢¼ï¼Œç¢ºä¿é€²å…¥é»ç›¸å®¹æ€§ï¼š
```python
# ç¢ºä¿ä¸»é€²å…¥é»å­˜åœ¨ (åˆ¥åæ›è¼‰)
if 'generate' not in globals() and any(k.startswith('generate_') for k in globals()):
    generate = next(v for k, v in globals().items() if k.startswith('generate_'))
``` """

    # åˆå§‹åŒ–è¨ˆæ•¸å™¨
    regex_fixes = 0
    logic_fixes = 0
    ast_repairs = 0
    prompt_tokens = 0
    completion_tokens = 0

    try:
        if current_app: current_app.logger.info(f"Generating {skill_id} with {current_model}")
        
        client = get_ai_client(role='coder') 
        # 1. å–å¾— LLM åŸå§‹å›è¦† (æ””æˆªé»)
        
        # æ¨¡æ“¬ ai_wrapper å›å‚³ (å…§å®¹, tokens) çš„è¡Œç‚º
        # é€™è£¡å‡è¨­ä½ çš„ get_ai_client å›å‚³çš„ client ä»ç„¶æ˜¯ google.generativeai çš„ç‰©ä»¶
        response = client.generate_content(prompt)
        raw_response = response.text
        
        # [V9.8] å˜—è©¦ç²å– Token ç”¨é‡
        try:
            if hasattr(response, 'usage_metadata'):
                prompt_tokens = response.usage_metadata.prompt_token_count
                completion_tokens = response.usage_metadata.candidates_token_count
        except:
            pass

        raw_len = len(raw_response)
        
        # 2. å•Ÿå‹•è‡ªç™’æµæ°´ç·šèˆ‡è¨ˆæ™‚
        healing_start = time.time()
        
        processed_code = raw_response
        
        # ç°¡å–®æ¸…ç† markdown
        match = re.search(r'```(?:python)?\s*(.*?)```', processed_code, re.DOTALL | re.IGNORECASE)
        if match: processed_code = match.group(1)
        elif "import random" in processed_code: processed_code = processed_code[processed_code.find("import random"):]
        
        # æ ¹æ“šå¯¦é©—çµ„åˆ¥ (ablation_id) æ±ºå®šä¿®å¾©å¼·åº¦
        # 1: Bare (ä¸ä¿®å¾©) | 2: Regex Only | 3: Full Healing (Regex + AST)
        
        final_code = processed_code
        
        if ablation_id >= 2:
            # Regex Armor
            final_code = inject_perfect_utils(final_code)
            
            # [V9.8.2 Defense] Hard Validation for 7B Models
            # validate_and_fix_code åŒ…å«äº† regex ä¿®å¾©
            final_code, pre_fixes = validate_and_fix_code(final_code)
            regex_fixes += pre_fixes

            final_code, patch_fixes = universal_function_patcher(final_code)
            regex_fixes += patch_fixes
            
            final_code = fix_return_format(final_code)
            final_code = clean_global_scope_execution(final_code)
            final_code = inject_robust_dispatcher(final_code) 
            final_code = fix_missing_answer_key(final_code)
            
        
        if ablation_id == 3:
            # Full Healing (AST + Logic)
            # [V9.8] é©—è­‰èˆ‡ä¿®å¾©
            is_valid, syntax_err = validate_python_code(final_code)
            if not is_valid:
                final_code, r_count = fix_code_syntax(final_code, syntax_err)
                regex_fixes += r_count # Count this as regex/syntax fix
                ast_repairs += 1 # Count as a repair event
                
            is_valid_log, logic_err = validate_logic_with_pyflakes(final_code)
            if not is_valid_log:
                final_code, l_count = fix_logic_errors(final_code, logic_err)
                logic_fixes += l_count
                ast_repairs += 1 # Count as a repair event

            # Final Logic Hardening
             # 1. String Deduplication
            if final_code.count("è«‹è¼¸å…¥") > 1 or final_code.count("ä¾‹å¦‚ï¼š") > 1:
                final_code = re.sub(r'(\(è«‹è¼¸å…¥.*?\))(\s*\\n\1)+', r'\1', final_code)
            
             # 2. Quote Hardening
            font_pattern = r"(?:matplotlib\.|plt\.)?rcParams\[['\"]font\.sans-serif['\"]\]\s*=\s*(?:\[[^\]]*\]|['\"].*?['\"])"
            final_code = re.sub(font_pattern, "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei']", final_code)
            
             # 3. Physical Newline Hardening
            final_code = final_code.replace('\\\\n', '\\n')


        healing_duration = time.time() - healing_start

        # 3. å¯¦é©—è©•åˆ†ï¼šèªæ³•æ­£ç¢ºæ€§æ ¡é©— (score_syntax)
        try:
            ast.parse(final_code)
            score_syntax = 100.0
        except SyntaxError:
            score_syntax = 0.0
            
        # å¯«å…¥æª”æ¡ˆ
        created_at = time.strftime('%Y-%m-%d %H:%M:%S')
        header = f'''# ==============================================================================
# ID: {skill_id}
# Model: {current_model} | Strategy: {strategy_name}
# Duration: {time.time() - start_time:.2f}s | RAG: {rag_count} examples
# Created At: {created_at}
# Fix Status: Ablation={ablation_id}
#==============================================================================\n\n'''
        path = os.path.join(current_app.root_path, 'skills', f'{skill_id}.py')
        with open(path, 'w', encoding='utf-8') as f:
            f.write(header + final_code)

        # 4. å‘¼å«æ›´æ–°å¾Œçš„ log_experiment (ç§‘ç ”å°æ¥)
        log_experiment(
            skill_id=skill_id,
            start_time=start_time,
            prompt_len=len(prompt),
            code_len=len(final_code),
            is_valid=(score_syntax == 100.0),
            error_msg="None" if score_syntax == 100.0 else "Syntax Error",
            repaired=(ast_repairs > 0 or regex_fixes > 0 or logic_fixes > 0),
            model_name=current_model,
            actual_provider=current_provider,
            # --- å‚³å…¥ç§‘ç ”å°ˆç”¨ kwargs ---
            model_size_class=model_size_class,
            prompt_level=prompt_level,
            raw_response=raw_response,       # å­˜ä¸‹ AI çš„ã€ŒåŸå§‹å¹»è¦ºã€
            final_code=final_code,           # å­˜ä¸‹ä½ çš„ã€Œé†«ç™‚æˆæœã€
            score_syntax=score_syntax,
            healing_duration=healing_duration,
            ablation_id=ablation_id,
            ast_repair_count=ast_repairs,
            regex_fix_count=regex_fixes,
            logic_fix_count=logic_fixes,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            resource_cleanup_flag=True # æ¨™è¨˜è³‡æºé‡‹æ”¾
        )

        return True, "Success"

    except Exception as e:
        # å³ä½¿å´©æ½°ä¹Ÿè¦ç´€éŒ„ï¼Œé€™å°åˆ†ææ¨¡å‹ç©©å®šæ€§éå¸¸é‡è¦
        log_experiment(
            skill_id=skill_id,
            start_time=start_time,
            prompt_len=0,
            code_len=0,
            is_valid=False,
            error_msg=str(e),
            repaired=False,
            model_name=current_model if 'current_model' in locals() else "Unknown",
            raw_response=raw_response if 'raw_response' in locals() else "LLM API Failure",
            ablation_id=ablation_id,
            model_size_class=model_size_class,
            prompt_level=prompt_level
        )
        return False, str(e)
